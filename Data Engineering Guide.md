# What is data engineering :
- Data engineering is a set of operations to make data available and usable to data scientists, data analysts, business intelligence (BI) developers, and other specialists within an organization. It takes dedicated experts – data engineers – to design and build systems for gathering and storing data at scale as well as preparing it for further analysis.

# Data engineering process
- The data engineering process covers a sequence of tasks that turn a large amount of raw data into a practical product meeting the needs of analysts, data scientists, machine learning engineers, and others. Typically, the end-to-end workflow consists of the following stages.
  
![data_engineering_image](https://www.altexsoft.com/static/blog-post/2023/11/41981453-7655-4463-9c06-cb6e80b69d04.webp)

- Data flow orchestration provides visibility into the data engineering process, ensuring that all tasks are successfully completed. It coordinates and continuously tracks data workflows to detect and fix data quality and performance issues.
### The mechanism that automates ingestion, transformation, and serving steps of the data engineering process is known as a data pipeline.

  1. Data ingestion (acquisition) moves data from multiple sources — SQL and NoSQL databases, IoT devices, websites, streaming services, etc. — to a target system to be transformed for further analysis. Data comes in various forms and can be both structured and unstructured.
 
  2. Data transformation adjusts disparate data to the needs of end users. It involves removing errors and duplicates from data, normalizing it, and converting it into the needed format.
 
  3. Data serving delivers transformed data to end users — a BI platform, dashboard, or data science team.
